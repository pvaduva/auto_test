#!/usr/bin/env expect-lite

# How to use this expect-lite file, Lines that begin with:
#       '>' send to remote host, implies "wait for prompt"
#       '<' _MUST_ be received from the remote host, or this config script will fail
#       # are comment lines, and have no effect
#       ; are printable (in stdout) comments, and have no other effect
#       @ change the expect timeout value
#       ! Embedded Expect commands
# For more info see: expect-lite.html

#
#       
#       
#
#
;;;

US49035- VM Scale Up/Down Script

Script can also be used as a scaling test:
        ./ HOST_IP=t_us49035_ts1_vcpu_scale_up_down.elt env.NODE.target.default.targetIP
Assumptions:
        Lab is setup with lab_setup.sh with Tenants
        heat template available in the switch

;;;



*NOFAIL
*EXP_INFO

# Variables defined to be used as contants
$HOST_IP=128.224.150.199
$HOST_USER=wrsroot
$PASS=li69nux
$NAT_IP=128.224.150.11
$NAT_USER=cgcs
$VM_USER=root

$tenant_credentials=/home/wrsroot/openrc.tenant1
$tenant=$tenant_credentials
# trim to just last part
=$tenant;.*[.](\w+);\1;

$lab_setup=yes

>date +%F_%T
+$DATE=\n(2.+)

@2

; === connecting to controller

>ssh -X -o UserKnownHostsFIle=/dev/null -o StrictHostKeyChecking=no $HOST_USER@$HOST_IP
-<WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED
<ssword:|Last login:
>>$PASS
>export TMOUT=0

; === become admin
>source /etc/nova/openrc

; === show version
>system show
>cat /etc/build.info

; === show system status
>nova service-list

; === validate that controllers and computes are present
>system host-list
<controller-0
<controller-1
<compute-\d
<compute-\d
# if computes are not present, stop the script
+$last_compute=(compute)
?if $last_compute!=compute? [
        ;red --- Looks like all the nodes are not online, please fix
        *TERM 5
]

; === Do valid lab_Setup
?if $lab_setup == yes? [
        ; === validate lab_setup script
        >ls lab_setup.sh
        <\nlab_setup.sh
        >ls lab_setup.conf
        <\nlab_setup.conf

        ; === use lab_setup to configure compute nodes, interfaces, provider networks, tenant networks, images, flavors. Ready to boot VMs
        @600
        >time ./lab_setup.sh
        #<Creating volume
        #<Creating volume
        #<Writing VM boot commands
        <\nreal

        @10
        ; === unlock computes
        $host=compute-0
        >system host-unlock $host
        $host=compute-1
        >system host-unlock $host

        $i=0
        $max=100
        >system host-list | grep $host
        +$avail=(online|offline|available|degraded)
        [ $avail != available
                >system host-list | grep $host
                +$avail=(online|offline|available|degraded)
                !sleep 10
                +$i
                ?if $i > $max? %BREAK_UNLOCK_CONTROLLER
        ]
        %BREAK_UNLOCK_CONTROLLER
]
>system host-list

; === Create necessary flavors and set minimum vcpu value
$flavor1_min_cpu=1
$flavor2_min_cpu=2
$flavor1_max_cpu=4
$flavor2_max_cpu=5
$flavor1=vcpu4-$flavor1_min_cpu
$flavor2=vcpu5-$flavor2_min_cpu


#Create flavor1
>nova flavor-create --dedicated-cpus True $flavor1 5f0d748b-1c8b-4a70-1985-4d57e25ac137 512 0 $flavor1_max_cpu
>echo $?
<\n0
#Create flavor2
>nova flavor-create --dedicated-cpus True $flavor2 db9b37f5-cba5-1985-a264-a6c1375a4ff7 512 0 $flavor2_max_cpu
>echo $?
<\n0
; === flavor created
>nova flavor-list | grep $flavor1
>nova flavor-list | grep $flavor2

#Set min vcpu value
; === set minimum value for the cpu created
>nova flavor-key $flavor1 set hw:wrs:min_vcpus=$flavor1_min_cpu 
>nova flavor-list --extra-specs | grep $flavor1
+$result=min.*vcpus.*(\d+)
; === Minimum CPU value set for $flavor1 is $result

>nova flavor-key $flavor2 set hw:wrs:min_vcpus=$flavor2_min_cpu
>nova flavor-list --extra-specs | grep $flavor2
+$result=min.*vcpus.*(\d+)
; === Minimum CPU value set for $flavor2 is $result
                                                                 
; === get tenant uuids
>keystone tenant-list
<name
+$tenant_uuid=([0-9a-f-]{32,36})\s* \| $tenant


; === get tenant id
>keystone user-list
>keystone user-list | grep $tenant
+$tenant_user_id=([0-9a-f-]{32,36})\s* \|\s+$tenant

# expand quotas for tenants
$quota_instances=30
$quota_vcpus=60
$quota_port=60
$quota_volumes=30

; === increase quotas for test
>nova quota-update --instances $quota_instances $tenant_uuid
>nova quota-update --cores $quota_vcpus $tenant_uuid
>nova quota-show --tenant $tenant_uuid
# must use tenant uuid when updating neutron quotas
>neutron quota-update --port $quota_port --tenant-id $tenant_uuid
>cinder quota-update --volumes $quota_volumes $tenant_uuid
>cinder quota-show $tenant_uuid

; === switch to tenant mode
>source $tenant_credentials

; === copy heat template to /home/wrsroot/ 
>find . -iname VMAutoScaling.yaml
+$result=(\n.*/hot/scenarios/VMAutoScaling.yaml)
>cp $result .
; === change the cool down period to 20 secs from 60 secs
>sed -i "s/Cooldown: '60'/Cooldown: '20'/g" VMAutoScaling.yaml
>sed -i "s/period: '60'/period: '120'/g" VMAutoScaling.yaml

$i=1
$MAX=10
[ $i<=$MAX
      $vm$i=guest$i
      +$i
]

#CGTS-905-Verify that Heat autoscaling still works after admin passwd change
; === CGTS-905-Verify that Heat autoscaling still works after admin passwd change
; === become admin to change tenant password
>source /etc/nova/openrc
>keystone user-password-update --pass tenantnew $tenant
>echo $?
<0

; === change openrc.tenant1 file with updated password
>sed -i "s/OS_PASSWORD=tenant1/OS_PASSWORD=tenantnew/g" $tenant_credentials

; === switch to tenant mode
>source $tenant_credentials
# Do scale up/down afer manual migration - lock compute
; Create STACK1 for manual migration

>heat stack-create -f VMAutoScaling.yaml -P TenantUserId=$tenant_user_id -P PrivateNetName=tenant1-mgmt-net -P InstanceName=guest1 -P FlavorName=$flavor1 -P ImageName=cgcs-guest -P CustomMeterName=vote -P KeyName=keypair-tenant1 STACK1
; === wait and check if heat stack1 create_complete
!sleep 45
>heat stack-show STACK1
<stack_status
-<CREATE_FAILED
<CREATE_COMPLETE

; === lock compute where VM is present
; === get ID of each instance
>nova list
>nova list | grep $vm1
+$resource_id_vm1=([0-9a-f-]{32,36})\s* \|\s+$vm1


>vm-topology | grep $resource_id_vm1
<$vm1
+$vm1_compute=(compute-\d)
$prev_vm1_compute=$vm1_compute

; === become admin to lock compute
>source /etc/nova/openrc

; === lock compute and check for migration 
>system host-lock $vm1_compute --force
>system host-list


; === wait for compute to be locked
$compute_state=none
[ $compute_state != locked
        >system host-list
        +$compute_state=$vm1_compute\s+\| compute\s+\| (locked|unlocked)
        +$i
        !sleep 5
]
>system host-list

; === switch back to tenant
>source ./openrc.tenant1
@20
>vm-topology | grep $resource_id_vm1
<$vm1
+$vm1_compute=(compute-\d)
?if $prev_vm1_compute==$compute_state? ;red === $vm1 did not migrate correctly after compute lock

################## Scale up/down via heat #######################
; === get ID of each instance
>nova list
>nova list | grep $vm$i
+$resource_id_vm$i=([0-9a-f-]{32,36})\s* \|\s+$vm$i


#Scale down guest1 via heat
$value=2
>ceilometer sample-create -r $resource_id_vm1 -m vote --meter-type gauge --meter-unit '%' --sample-volume $value --resource-metadata '{"display_name":"$vm1"}'
>ceilometer resource-show -r $resource_id_vm1
!sleep 30
>ceilometer alarm-list
<STACK1.*Low.*|
<alarm


; === going in a loop to check $vm1 reached minimum cpu vale 
$i=0
$max=20 
>nova show $vm1
<$vm1
+$min_cpu=(\d+),\s*\d+,\s*\d+
+$cur_cpu=\d+,\s*(\d+),\s*\d+
+$max_cpu=\d+,\s*\d+,\s*(\d+)
[ $cur_cpu != $min_cpu
	   >nova show $vm1
           <$vm1
           +$cur_cpu=\d+,\s*(\d+),\s*\d+
           !sleep 30
	   ; === auto scaling down
           +$i
           ?if $cur_cpu == $min_cpu? %BREAK_REACHED_MIN_CPU20
]
%BREAK_REACHED_MIN_CPU20

; === check if $vm1 current cpu value reached minimum cpu value:$min_cpu
>nova show $vm1
<(min/cur/max)
+$min_cpu=(\d+),\s*\d+,\s*\d+
+$cur_cpu=\d+,\s*(\d+),\s*\d+
+$max_cpu=\d+,\s*\d+,\s*(\d+)

?if $cur_cpu!=$min_cpu? [
        ;red === $vm1 cpu value did not reached minimum value $min_cpu
]

#scale up guest1 via heat 
$value=90
>ceilometer sample-create -r $resource_id_vm1 -m vote --meter-type gauge --meter-unit '%' --sample-volume $value --resource-metadata '{"display_name":"$vm1"}'
>ceilometer resource-show -r $resource_id_vm1
!sleep 30
>ceilometer alarm-list
<STACK1.*High.*|
<alarm
; === going in a loop to check $vm1 reached maximum cpu vale 
$i=0
$max=10 
>nova show $vm1
<$vm1
+$min_cpu=(\d+),\s*\d+,\s*\d+
+$cur_cpu=\d+,\s*(\d+),\s*\d+
+$max_cpu=\d+,\s*\d+,\s*(\d+)
[ $cur_cpu != $max_cpu
	   >nova show $vm1
           <$vm1
           +$cur_cpu=\d+,\s*(\d+),\s*\d+
           !sleep 30
	   ; === auto scaling up
           +$i
           ?if $cur_cpu == $max_cpu? %BREAK_REACHED_MAX_CPU20
]
%BREAK_REACHED_MAX_CPU20

; === check if $vm1 current cpu value reached maximum cpu value:$max_cpu
>nova show $vm1
<(min/cur/max)
+$min_cpu=(\d+),\s*\d+,\s*\d+
+$cur_cpu=\d+,\s*(\d+),\s*\d+
+$max_cpu=\d+,\s*\d+,\s*(\d+)

?if $cur_cpu!=$max_cpu? [
        ;red === $vm1 cpu value did not reached maximum value $max_cpu
]
#complete guest1 scale up/down via heat

; === become admin to lock compute
>source /etc/nova/openrc

; === lock compute and check for migration 
>system host-unlock $prev_vm1_compute 
>system host-list
<$prev_vm1_compute
; === wait for compute to be unlocked
$compute_state=none
[ $compute_state != unlocked
        >system host-list
        +$compute_state=$prev_vm1_compute\s+\| compute\s+\| (locked|unlocked)\s+\| (enabled|disabled)\s+\| (online|available)
        +$i
        !sleep 5
]

; === become admin to revert tenant password
>source /etc/nova/openrc
>keystone user-password-update --pass tenant1 $tenant
>echo $?
<0

; === change openrc.tenant1 file with updated password
>sed -i "s/PASSWORD=tenantnew/PASSWORD=tenant1/g" $tenant_credentials
; === switch to tenant mode
>source $tenant_credentials


### CGTS-905scale up/down after tenant password change complete
####complete scale up/down after manual migration 




################## Clean up #############################################
; === become admin to clean up
>source /etc/nova/openrc
;cleanup all setup created for this testcase
; === delete the stack created

; === delete the flavor created 
>nova flavor-list
>nova flavor-delete $flavor1
>nova flavor-delete $flavor2
>nova flavor-list
-<$flavor1
-<$flavor2
; === become tenant to clean up stack
>source $tenant_credentials
; === delete heat stack created
>heat stack-delete STACK1 
!sleep 30
>heat stack-list
-<STACK1



